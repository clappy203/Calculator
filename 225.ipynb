{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "225.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30731HHrB6nY"
      },
      "source": [
        "# Module 2: Data Engineering\n",
        "## Sprint 2: SQL and Data Scraping\n",
        "## Storing Scraped Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAjjSrLjB6ni"
      },
      "source": [
        "## Background \n",
        "You did an outstanding job this sprint. Now you know SQL databases, how to make basic data operations. You are familiar with the pros and cons of SQL and should know when to choose a NoSQL database. Data scraping is a technique that you can use to create your own dataset. You are also familiar with the basic concepts of Spark. You should be proud of yourself! For the last lesson of this week, you should put all your learnings into one place to collect, process, and store data. Combining these two might require a bit of planning at first but this is what you will need to do now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_9yIl1IB6nj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY4OWtEaB6nk"
      },
      "source": [
        "## Creating the PostgreSQL database\n",
        "First, you will need to create a database that you will need to use to store the data you will collect. Follow the steps provided in the second lesson of this sprint. Do not forget to remove all secrets and passwords when committing code to the repository. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHYd_DjkB6nk"
      },
      "source": [
        "## Scraping the data\n",
        "This lesson, you will need to scrape and store data collected from an online store. You can choose any website that you like (Vinted, Amazon, eBay, etc.). You will need to select three keywords (for example `dress`, `bike`, `bracelet`) and scrape listings from the selected website. You will need to collect at least 3000 samples for each category and store this information of the listing: `category`, `title`, `price`, `url to item`, `url of image`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWX2A_TB6nl"
      },
      "source": [
        "## Structure of the database\n",
        "As you will be storing data into the relational database, you will need to create tables. There should be two tables with many to one type of relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTRvBaBaB6nl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1jLv4GhB6nm"
      },
      "source": [
        "## Concepts to explore\n",
        "* Creating relational type database\n",
        "* Creating PostgreSQL database in Heroku\n",
        "* Writing SQL queries\n",
        "* Scraping webpages using Beautiful Soup\n",
        "* Storing and acquiring data using SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDoqUvp2B6nn"
      },
      "source": [
        "## Requirements\n",
        "You should create two files: one that contains functions related to data scraping and another that is created to enable communication to PostgreSQL database created with Heroku. The actual requirements are these:\n",
        "- Database should be created using Heroku.\n",
        "- Required tables should be created. Python code should be provided inside `.py` file.\n",
        "- Scraping functions should be created. The main scraping function should take two arguments: `number of examples to scrape` and `keyword to search`. The main scraping function should return a Pandas `DataFrame` with the records.\n",
        "- Scrape the website. Get minimum 3000 samples of each category (keyword)\n",
        "- Data should be inserted into tables of database hosted by Heroku. Provide screenshots proofing that data sits inside the database.\n",
        "- Join two tables into one using SQL query and export it to `csv` file. Provide a function that makes this action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCqRy2VDB6no"
      },
      "source": [
        "## Evaluation criteria\n",
        "1. The requirements are met (database and tables in it are created, data is scraped and stored)\n",
        "2. Data scraping functions are written.\n",
        "3. Code meets expected standards (type hints, PEP8 standards)\n",
        "4. Documentation is provided (comments are written where needed, README.md file is created)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pv9wRX2B6np"
      },
      "source": [
        "\n",
        "## Sample correction questions\n",
        "\n",
        "During a correction, you may get asked questions that test your understanding of covered topics.\n",
        "\n",
        "- Why do we store information inside external databases? Tell two advantages and disadvantages when comparing local and external storage.\n",
        "- What is the difference between SQL and NoSQL databases? Tell two examples: one where you would want to use PostgreSQL database and another where you would choose Cassandra.\n",
        "- How to properly set up a data scraping strategy? What are the key steps that you must make to successfully create a dataset using web scraping technique?\n",
        "- Why do we need Spark? Tell about one use case where Spark would improve scalability and speed of queries when comparing to traditional relational type database's engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GF3a_puXLHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291a5f41-3dcf-4faa-c01e-fac07cdec334"
      },
      "source": [
        "!pip install BeautifulSoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbAI4-VNB6nq"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJZ2eodXoDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d07209-f233-4160-bf15-1f2967fcd8a4"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2499334.m570.l1313&_nkw=phones+&_sacat=0\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "page"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [504]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UTBY389iuuC"
      },
      "source": [
        "sample_size = 3000\n",
        "keyword = 'phones'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdw9B6GhHfmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e4012f-b679-439b-df01-49e1189c627f"
      },
      "source": [
        "import pprint\n",
        "\n",
        "\n",
        "page_url = []\n",
        "for i in range(1,round(sample_size/200)):\n",
        "   page_url.append(f\"https://www.ebay.com/sch/i.html?_from=R40&_nkw={keyword}+&_sacat=0&_ipg=200&_pgn={i}\")\n",
        "pprint.pprint(page_url)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=1',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=2',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=3',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=4',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=5',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=6',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=7',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=8',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=9',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=10',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=11',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=12',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=13',\n",
            " 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=phones+&_sacat=0&_ipg=200&_pgn=14']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGm79nCYpcSB"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "for links in page_url:\n",
        "   page = requests.get(links, headers=headers)\n",
        "   soup = BeautifulSoup(page.content, \"html.parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2zUcssgpg_z",
        "outputId": "e6ea8206-5a15-4209-840b-5665425d62ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len (page_url)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYMs3RklO5je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0424fd77-9c1e-4109-8278-35cea97cda8b"
      },
      "source": [
        "containers = soup.find_all(\"div\", {\"class\": \"s-item__wrapper clearfix\"})\n",
        "len(containers)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiMkV3F6TShG"
      },
      "source": [
        "containers[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr7Dy5jaRDtP"
      },
      "source": [
        "for price in soup.find_all(\"h3\", class_=\"s-item__title\"):\n",
        "    print(title.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZK4rxsaMvJf"
      },
      "source": [
        "for item in soup.find_all(\"span\", class_=\"data-spm-anchor-id\"):\n",
        "    print(item.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmzfDbNcBwdY"
      },
      "source": [
        "for links in page_url:\n",
        "    page = requests.get(links, headers={\"User-Agent\": \"ua\"})\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    for item in soup.find_all(\"a\", class_=\"_2lsU7\"):\n",
        "      print(item.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3GxZ84YFGcc"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "for title in soup.find_all(\"a\", class_=\"storylink\"):\n",
        "    print(title.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0itF7sHy0lfo"
      },
      "source": [
        "keywords= [\"phones\", \"shoes\", \"cloths\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0gaH9qsEtJz"
      },
      "source": [
        "demo = []\n",
        "for keyword in keywords:\n",
        "  demo.extend(get_url(keyword, 3000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1M49kA8oZlX"
      },
      "source": [
        "get_url(\"phones\", 3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR1WF_LX2GYa"
      },
      "source": [
        "demo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE4-DsH0gkLP"
      },
      "source": [
        "_588b5_3MtNs  / _588b5_3MtNs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qKLeY90gxVx"
      },
      "source": [
        "_58c31_2R34y _00c44_25nJl section "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}