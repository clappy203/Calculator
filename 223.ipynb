{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "223.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFlBduTjBJCS"
      },
      "source": [
        "# Module 2: Data Engineering\n",
        "## Sprint 2: SQL and Data Scraping\n",
        "## Subproject 3: Web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1U94O2vBJCb"
      },
      "source": [
        "During this lesson, you will learn all that you need to know to start scraping the internet. You will get familiar with the structure of websites, key elements of HTML. You will be introduced to the `requests` and `bs4` libraries that combined enable robust web scraping workflow that can be used to create datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R27SPlp0BJCe"
      },
      "source": [
        "## Learning outcomes\n",
        "- At the end of this lesson you will be able to create web page's scraping strategy\n",
        "- You will be able to understand HTML document's structure\n",
        "- You will know how to extract specific information form websites\n",
        "- You will know how to scrape websites to create your own datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKySIhsBJCg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LszSWLZKBJCi"
      },
      "source": [
        "## Why we scrape the internet?\n",
        "Data science without data is not much of a science. In most cases, you can find many great curated clean datasets online (for example Kaggle). Sometimes you need specific data for your project to succeed. How can you collect one? Well, you can always search the web for particular information and manually create your own dataset. Unfortunately, these days many models require quite large data samples and it would be time-consuming and sometimes even impossible to manually collect +10k individual samples to your dataset. Luckily for us, there is one great method that enables us to automate this process. It is called data scraping. This technique is widely used by software agents for different purposes. One of the examples is Google. The software giant goes to every page, scans it, and puts the information into its databases for later indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ykVQcZpBJCj"
      },
      "source": [
        "## What is a Webpage?\n",
        "Almost all web pages are text documents presented in `.html` format. HTML is a markup language similar to Markdown that you use to write README files on GitHub. To be able to scrape information from HTML files, first, you need to understand how to create one. You have to watch [this](https://www.youtube.com/watch?v=pQN-pnXPaVg) freecodecamp video that explains the main concepts of creating **HTML based** website. Later, we will take a closer look into individual subjects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3MxI9ZGBJCl"
      },
      "source": [
        "## Anatomy of an HTML element\n",
        "### Nesting elements\n",
        "Every HTML element lives inside other HTML element. HTML documents have their own strict structure. Here is a base skeleton of every HTML document:\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Page Title</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>My First Heading</h1>\n",
        "        <p>My first paragraph.</p>\n",
        "    </body>\n",
        "</html> \n",
        "```\n",
        "As you can see there are three main parts of the HTML document:\n",
        "* `<html>` - place where all html content rests in\n",
        "* `<head>` - place where meta information of page is placed: title, scripts, stylesheets, etc.\n",
        "* `<body>` - the main part of the document: all information is placed here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQVZ5m4qBJCo"
      },
      "source": [
        "### Tags, Classes and Ids\n",
        "As you will primarily be working with getting information from HTML documents, let's talk about what you can find inside `<body>` part. All information inside HTML documents is presented via [html tags](https://developer.mozilla.org/en-US/docs/Web/HTML/Element). There is a large number of HTML tags available but almost all of them share the same properties: they are place of some block inside the document and they format presented information. For example:\n",
        "```html\n",
        "<div class=\"content\">\n",
        "    <h1>Text</h1>\n",
        "    <h2 id=\"subtxt\">Subtext</h2>\n",
        "</div>\n",
        "```\n",
        "As you can see, elements are nested inside other elements. Information is presented inside HTML tags. HTML elements can be differentiated using **class** and **id**. Classes are used to group certain elements and apply scripts and styles to them. The same is with ids but the idea is that ids should be unique per document but sadly this is not always the case. Why tags and classes are important to us? We can use them to select parts of the HTML element that we want to extract information from. Let's say there is a page where capitals and native language of EU countries are listed:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWv7W0tBJCr"
      },
      "source": [
        "```html\n",
        "<div>\n",
        "    <ul class=\"capitals\">\n",
        "        <li>Vilnius</li>\n",
        "        <li>Paris</li>\n",
        "        <li>Riga</li>\n",
        "        <li>Tallinn</li>\n",
        "    </ul>\n",
        "    <ul class=\"languages\">\n",
        "        <li>Lithuanian</li>\n",
        "        <li>French</li>\n",
        "        <li>Latvian</li>\n",
        "        <li>Estonian</li>\n",
        "    </ul>\n",
        "</div>\n",
        "```\n",
        "If you just select `li` as the tag you want to extract information from you will get all: capitals and languages but you want only capitals. `ul` selection also would not give an expected result as there are two `ul` lists in the document. What you want to do is to select `.capitals ul` - by providing class we are able to select distinct `ul` and extract wanted information.\n",
        "\n",
        "[Here](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML) is a full introduction to HTML made Mozilla. You can complete all of it if you want to get deep knowledge in the subject. If you want just do some web scraping, knowledge of the HTML file structure and its elements is more than enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGN5YSmRBJCv"
      },
      "source": [
        "## Web Scraping\n",
        "As mentioned in the beginning of the lesson, web scraping is used to automate information collection from websites and to create datasets for later usage. There are many great tools that can be used to scrape websites. Most popular ones are:\n",
        "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "* [Scrapy](https://scrapy.org/)\n",
        "* [Selenium](https://www.selenium.dev/)\n",
        "\n",
        "Scrapy is a more advanced tool that need a bit more setup, Selenium is really similar to Beautiful Soup but Beautiful Soup is more popular and easier to use option, so in this lesson we will be scraping websites using this tool.\n",
        "First, you should watch [this freecodecamp video](https://www.youtube.com/watch?v=87Gx3U0BDlo) about Soup that covers all basic usages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWrEa3J4BJCx"
      },
      "source": [
        "## Setup\n",
        "Setup of Soup is simple and does not require any difficult steps to be made. Just use `pip` to install it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3z0JVj9BJCz",
        "outputId": "b84d2698-8e31-47d1-ed7b-acc70b485cd4"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7VqNdNhBJC4"
      },
      "source": [
        "Now we can use it to extract information from HTML elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ3FalrJBJC6",
        "outputId": "e31399d6-cf1b-4fff-8508-6db58511ddc7"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Defining some HTML format text\n",
        "html_doc = \"\"\"\n",
        "<html>\n",
        "<head><title>Example text</title></head>\n",
        "<body><p class=\"text\">Text to extract</p></body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Creating soup object using BeutifulSoup\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
        "\n",
        "# Finding all elements inside HTML that have p tag and text class\n",
        "p_results = soup.find_all(\"p\", class_=\"text\")\n",
        "\n",
        "# Extracting text value from found elements\n",
        "for p_result in p_results:\n",
        "    print(p_result.text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to extract\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlH9hUF4BJC8"
      },
      "source": [
        "As you can see from the example, we successfully found desired elements in the HTML document and extracted text values from them. This is pretty neat but the power of web scraping comes from automation and real-world examples. So now let's talk about how to extract information from real web pages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gmKxDZDBJC8"
      },
      "source": [
        "## Downloading pages\n",
        "To get HTML format document to extract data from you need to download it. It is not that difficult if you are using Python `requests`. We will use a popular forum [Hacker News](https://news.ycombinator.com/) as a website that we will scrape information from:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQcR6UXeBJC9",
        "outputId": "677b0149-6223-4463-d15f-a68ce45f0895"
      },
      "source": [
        "import requests\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "page"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9oGrucBBJC-"
      },
      "source": [
        "Some websites can be protected from web scraping. In this case, you need to find ways of exploiting their systems. Most of the time addition of `User-Agent` header in the request should do the thing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTvZvE0OBJC-",
        "outputId": "b02fae3f-46cf-40a9-bf28-ef06791b49ba"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "page"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv-POMU_BJC_"
      },
      "source": [
        "## Looking for text\n",
        "Now that we have a page downloaded we can start extracting information from it. First, we need to find exact parts of the page we want to get text from. We can do it by going to the web page and using [inspect element](https://zapier.com/blog/inspect-element-tutorial/) functionality of your browser. Let's say we want to collect titles of all posts in the page:\n",
        "<div><img src=\"https://i.imgur.com/HJ8DwAN.png\" width=\"600px\"/></div>\n",
        "We can see that information that we need is inside the `<a>` tag with the `storylink` class. We can use `soup` to extract the information by providing newly learned properties of the elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZn0Trr6BJDA",
        "outputId": "caf78fbd-7ca8-4061-d14a-430a42ec2e53"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "for title in soup.find_all(\"a\", class_=\"storylink\"):\n",
        "    print(title.text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I nearly lost the Lightroom catalog with of all my photos\n",
            "Writing Programs with Ncurses\n",
            "Guide to Firefox Containers\n",
            "Music Composition with Deep Learning: A Review\n",
            "Anthropic Bias (2020)\n",
            "Stop Waiting for Godot\n",
            "Monaru (YC S19) Is Hiring an iOS Engineer\n",
            "MarkMonitor left 60k domains for the taking\n",
            "Racer Trash\n",
            "Nvui: A NeoVim GUI written in C++ and Qt\n",
            "Gap Inc. acquires 3D fitting room startup Drapr\n",
            "Pico8Lisp: A toy Lisp interpreter for PICO-8\n",
            "In Go, pointers (mostly) don't go with slices in practice\n",
            "Stanford researchers make rechargeable batteries that store 6x more charge\n",
            "Show HN: A strongly-typed document DB that runs on any transactional KV store\n",
            "How to fight microplastic pollution with magnets\n",
            "The Oasis of Palmyra\n",
            "Satellites Spot Oceans Aglow with Trillions of Organisms\n",
            "When AWS, Azure, or GCP Becomes the Competition (2019)\n",
            "Helm is a personal server that lives where you do\n",
            "Connection Strings Reference\n",
            "The Fatiguing Effects of Camera Use in Virtual Meetings [pdf]\n",
            "We Can’t Trust the Market with Scientific Knowledge\n",
            "Three Clocks Are Better Than One\n",
            "Building a Desktop Application for Datasette\n",
            "8 Bits of history: My first game is still available on the internet\n",
            "Software Development Waste\n",
            "Open Source Illustrations Kit\n",
            "Show HN: We built an end-to-end encrypted alternative to Google Photos\n",
            "Latin Literature’s Problem with Invisibility\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0VH6vwxBJDA"
      },
      "source": [
        "## Storing collected data\n",
        "You can use collected data to create pandas DataFrames. This enables you to make various data processing operations (removing corrupted information, filling in missing data). Pandas will also make your life easier when you will need to export collected information to `csv` files or insert data to databases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54X8LwoABJDB"
      },
      "source": [
        "# Run this if needed\n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "JO4VkBIaBJDB",
        "outputId": "6ff2fb17-2e56-4a68-82a2-075eb46b2cb2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "page = requests.get(\"https://news.ycombinator.com/\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "collected_information = [{\"title\": title.text} for title in soup.find_all(\"a\", class_=\"storylink\")]\n",
        "\n",
        "df = pd.DataFrame(collected_information)\n",
        "df.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Writing Programs with Ncurses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guide to Firefox Containers (2018)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Anthropic Bias (2020)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Music Composition with Deep Learning: A Review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Stop Waiting for Godot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            title\n",
              "0                   Writing Programs with Ncurses\n",
              "1              Guide to Firefox Containers (2018)\n",
              "2                           Anthropic Bias (2020)\n",
              "3  Music Composition with Deep Learning: A Review\n",
              "4                          Stop Waiting for Godot"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZD9zmmBJDC"
      },
      "source": [
        "You can see how powerful `soup` is for doing web scraping tasks! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24vvoVMBJDC"
      },
      "source": [
        "## Exercise\n",
        "Now it is your time to shine: you will need to extract more information from the web page and put it inside pandas DataFrame. Write code inside the cells for tests below to pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfY9WeyYBJDD"
      },
      "source": [
        "# Get score of every element\n",
        "collected_scores =[{\"score\": span.text} for span in soup.find_all(\"span\", class_=\"score\")]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3J7jopbBJDD"
      },
      "source": [
        "assert type(collected_scores[0]) == dict\n",
        "assert list(collected_scores[0].keys())[0] == \"score\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjkopr70BJDE"
      },
      "source": [
        "# Get age of the post\n",
        "collected_age = [{\"post_age\": span.text} for span in soup.find_all(\"span\", class_=\"age\")]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q1455egBJDF"
      },
      "source": [
        "assert type(collected_age[0]) == dict\n",
        "assert list(collected_age[0].keys())[0] == \"post_age\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCgrcmwnBJDF"
      },
      "source": [
        "# Get post's author.\n",
        "collected_users= [{\"post_author\": span.text} for span in soup.find_all(\"a\", class_=\"hnuser\")]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z55c95XSBJDF"
      },
      "source": [
        "assert type(collected_users[0]) == dict\n",
        "assert list(collected_users[0].keys())[0] == \"post_author\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sk3L0dbBJDG"
      },
      "source": [
        "Now that you are able to extract information from the page, you need to automate this process and extract information from multiple pages at once and add the information to one `dict` (appending it to list). You will need to complete a function that returns a dataframe with columns: title, link, and age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "PWKTRiALBJDG",
        "outputId": "8566ed76-7dde-4f65-fd31-3f6132dd2f66"
      },
      "source": [
        "df = collect_information(2)\n",
        "assert df.shape == (60, 3)\n",
        "assert df.columns[0] == \"title\"\n",
        "\n",
        "df = collect_information(5)\n",
        "assert df.shape == (150, 3)\n",
        "assert df.columns[2] == \"age\""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-55064f772d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'collect_information' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34NjJUyJBJDH"
      },
      "source": [
        "## Exercise\n",
        "This lesson for the sub-project you will need to scrape reddit's homepage. As it can be difficult to do it with the new Reddit's design, you can visit the old one instead - [old.reddit.com](https://old.reddit.com). You will need to complete these tasks:\n",
        "1. Visit old.reddit.com and look at its layout\n",
        "2. Create a function that can scrape pages of Reddit. It should return a dataset with: `post score`, `post title`, `post thumb URL`, `posts comments count`, `posts subreddit`. All the missing information should be replaced with `None` values\n",
        "3. Scrape website and create dataframe that has at least 300 rows\n",
        "4. Export dataframe's data to csv format and save it in your repository as `reddit_data.csv`\n",
        "\n",
        "**IMPORTANT**: You might want to check out [this](https://pypi.org/project/fake-useragent/) Python package that creates a fake user agent for the request.\n",
        "\n",
        "Do not forget to write clean code!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynSh3RQ3BJDH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZhhVthKBJDH"
      },
      "source": [
        "## Summary\n",
        "If you want to create your own dataset, collecting it by using web scraping technique is one of the ways of doing it. Sometimes data will not be presented in pretty preprocessed csv columns and you will need to extract it from various resources by yourself. Automation of mentioned process can sometimes be a key part of a successful ML Project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl1MyDa2BJDI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZKMm7lqBJDI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}